{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Assistant with DuckDuckGo API\n",
    "\n",
    "This Jupyter Notebook demonstrates how to interact with the DuckDuckGo chat API using asynchronous programming in Python. The script allows you to send messages to the chat assistant and receive responses in real-time. \n",
    "\n",
    "## Requirements\n",
    "\n",
    "To run this notebook, you need to have the following Python packages installed:\n",
    "\n",
    "- `httpx`: For making asynchronous HTTP requests.\n",
    "- `asyncio`: For handling asynchronous operations (included in Python standard library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "\n",
    "STATUS_URL = \"https://duckduckgo.com/duckchat/v1/status\"\n",
    "CHAT_URL = \"https://duckduckgo.com/duckchat/v1/chat\"\n",
    "STATUS_HEADERS = {\"x-vqd-accept\": \"1\"}\n",
    "\n",
    "class Model:\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    CLAUDE_3_HAIKU = \"claude-3-haiku-20240307\"\n",
    "    META_LLAMA = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n",
    "    MISTRALAI = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "class ModelAlias:\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    CLAUDE_3_HAIKU = \"claude-3-haiku\"\n",
    "    LLAMA = \"llama\"\n",
    "    MIXTRAL = \"mixtral\"\n",
    "\n",
    "class Chat:\n",
    "    def __init__(self, vqd: str, model: str):\n",
    "        self.old_vqd = vqd\n",
    "        self.new_vqd = vqd\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "\n",
    "    async def fetch(self, content: str) -> httpx.Response:\n",
    "        self.messages.append({\"content\": content, \"role\": \"user\"})\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": self.messages,\n",
    "        }\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.post(\n",
    "                CHAT_URL,\n",
    "                headers={\"x-vqd-4\": self.new_vqd, \"Content-Type\": \"application/json\"},\n",
    "                json=payload\n",
    "            )\n",
    "            if not response.is_success:\n",
    "                raise Exception(f\"{response.status_code}: Failed to send message. {response.text}\")\n",
    "            return response\n",
    "\n",
    "    async def fetch_full(self, content: str) -> str:\n",
    "        message = await self.fetch(content)\n",
    "        full_message = await self.stream_events(message)\n",
    "        self.old_vqd = self.new_vqd\n",
    "        self.new_vqd = message.headers.get(\"x-vqd-4\")\n",
    "        self.messages.append({\"content\": full_message, \"role\": \"assistant\"})\n",
    "        return full_message\n",
    "\n",
    "    async def fetch_stream(self, content: str):\n",
    "        self.messages.append({\"content\": content, \"role\": \"user\"})\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": self.messages,\n",
    "        }\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.post(\n",
    "                CHAT_URL,\n",
    "                headers={\"x-vqd-4\": self.new_vqd, \"Content-Type\": \"application/json\"},\n",
    "                json=payload\n",
    "            )\n",
    "            if not response.is_success:\n",
    "                raise Exception(f\"{response.status_code}: Failed to send message. {response.text}\")\n",
    "\n",
    "            # Now read the response as a stream\n",
    "            async for line in response.aiter_lines():\n",
    "                if line:\n",
    "                    line = line[len(\"data: \"):].strip()\n",
    "                    if line == \"[DONE]\":\n",
    "                        break\n",
    "                    try:\n",
    "                        json_data = json.loads(line)\n",
    "                        if \"message\" in json_data:\n",
    "                            yield json_data[\"message\"]  # Yield each message as it comes\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Skipping invalid JSON line: {line}\")\n",
    "\n",
    "\n",
    "    async def stream_events(self, message: httpx.Response):\n",
    "        full_message = \"\"\n",
    "        async for line in message.aiter_lines():\n",
    "            if line:\n",
    "                line = line[len(\"data: \"):].strip()\n",
    "                if line == \"[DONE]\":\n",
    "                    break\n",
    "                try:\n",
    "                    json_data = json.loads(line)\n",
    "                    if \"message\" in json_data:\n",
    "                        full_message += json_data[\"message\"]\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid JSON line: {line}\")\n",
    "        return full_message\n",
    "\n",
    "    def redo(self):\n",
    "        self.new_vqd = self.old_vqd\n",
    "        self.messages.pop()\n",
    "        self.messages.pop()\n",
    "\n",
    "async def init_chat(model: str) -> Chat:\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        status = await client.get(STATUS_URL, headers=STATUS_HEADERS)\n",
    "        vqd = status.headers.get(\"x-vqd-4\")\n",
    "        if not vqd:\n",
    "            raise Exception(f\"{status.status_code}: Failed to initialize chat. {status.text}\")\n",
    "        return Chat(vqd, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merhaba! Ben bir yapay zeka dil modeliyim, bu yüzden duygularım yok ama size yardımcı olmaktan memnuniyet duyarım. Siz nasılsınız?\n"
     ]
    }
   ],
   "source": [
    "# Example usage in a Jupyter notebook\n",
    "chat_instance = await init_chat(ModelAlias.GPT_4O_MINI)\n",
    "response = await chat_instance.fetch_full(\"Merhaba. Nasıl gidiyor?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
